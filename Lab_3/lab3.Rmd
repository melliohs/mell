<div align="center">
  <br>
  <img src="https://i.ibb.co/DkYw4wF/logo.png" alt="logo" border="0"><br><br>
  МИНОБРНАУКИ РОССИИ<br>
  Федеральное государственное бюджетное<br>
  образовательное учреждение высшего образования
  
#### **«МИРЭА – Российский технологический униврситет»**
  
  Лабораторная работа по дисциплине:<br>
  «Программные средства оперативно-аналитического поиска»
</div>
<br><br><br><br><br>
<div align="right">
  <br>
  Выполнил:<br>
  Студент 4 курса<br>
  Специальности 10.05.05<br>
  Группа ББСО-02-16<br>
  Поляков Д.А.<br>
  Проверил:<br>
  Захарчук И.И.<br>
</div>
<br><br><br><br><br><br><br>

<p align="center">
  Москва 2020
</p>
<br><br><br>

<div align="center">

#### **Лабораторная работа № 3**

</div>

<p>

#### **Цель работы**

Исследование возможностей автоматизации сбора данных о доменах. Сбор информации о топ 15 доменах в категорий Emulators.
</p>

<p>

#### **Исходные данные**

|             Наименование устройства              | Программа виртуализации |  Операционная система | 
| ------------------------------------------------ | ----------------------- | --------------------- |
|  MacBook Pro (Retina 13 дюймов, начало 2015 г.)  |   VMware Fusion Pro 15  |    Ubuntu 18.04.4     |
</p>

### Используемое ПО:
1.Rstudio IDE    
2.nmap      
3.dig        
4.whois   
5.whatweb

### Варианты решения задачи:
1.Собрать информацию вручную с помощью веб-браузера, инструментов whois, dig, nmap и т.д.    
2.Использовать интегрированные инструменты такие как SpiderFoot, Maltego CE, Datasploit, Recon-ng.    
3.Самостоятельно разработать (для образовательных целей) автоматизированное решение для сбора информации.    

### Содержание лабораторной работы:
В данной лабораторной работе используется 3 вариант решения задачи. Соберем необходимую информацию автоматизированным способом.

```{r cache=TRUE}
library(tidyverse)
get_sum_df <- function(company_url) {
  country_state <- NA
  dig <- system2('dig', company_url, stdout = TRUE)
  ip <- dig %>%
    grep(pattern = company_url, value = TRUE) %>%
    str_extract(pattern = "\\b(?:[0-9]{1,3}\\.){3}[0-9]{1,3}\\b")
  ip <- ip[!is.na(ip)]
 
  whois <- system2('whois', ip[1], stdout = TRUE)
  phones <- whois %>%
    grep(pattern = "Phone", value = TRUE, ignore.case = TRUE) %>%
    str_squish() %>%
    str_split(pattern = " ") %>%
    data.table::transpose() %>%
    .[[2]] %>%
    unique() %>%
    str_c(collapse = " ")
 
  netblock <- whois %>%
    grep(pattern = "CIDR", value = TRUE, ignore.case = TRUE) %>%
    str_squish() %>%
    str_split(pattern = " ", simplify = TRUE) %>%
    .[-1] %>%
    str_c(collapse = " ")
 
  country <- whois %>%
    grep(pattern = "Country",
         value = TRUE,
         ignore.case = TRUE) %>%
    str_squish() %>%
    str_split(pattern = " ", simplify = TRUE) %>%
    .[-1]
 
  country_state <- whois %>%
    grep(pattern = "State",
         value = TRUE,
         ignore.case = TRUE) %>%
    str_squish() %>%
    str_split(pattern = " ", simplify = TRUE) %>%
    .[-1]
  if(length(country_state)==0) country_state <- NA
 
  address <- whois %>%
    grep(pattern = "address",
         value = TRUE,
         ignore.case = TRUE) %>%
    str_squish() %>%
    str_split(pattern = " ", simplify = TRUE) %>%
    .[-1] %>%
    str_c(collapse = " ")
 
  hosting <- whois %>%
    grep(pattern = "Hosting",
         value = TRUE,
         ignore.case = TRUE) %>%
    str_squish() %>%
    str_split(pattern = " ")
  hosting <- lapply(hosting, collapse = " ", str_c) %>%
    str_c(collapse = " ")
 
  nmap <-
    system2('nmap',
            args = c('-p', '22,21,80,443', ip[1]),
            stdout = TRUE)
  ports <- nmap %>%
    grep(pattern = "open",
         value = TRUE,
         ignore.case = TRUE) %>%
    str_squish() %>%
    str_split(pattern = " ") %>%
    data.table::transpose() %>%
    .[[1]] %>%
    str_c(collapse = " ")
  ip <- str_c(ip,collapse = ' ')
 
  company_sum <-
    data.frame(
      csum = c(
        company_url,
        ip,
        netblock,
        country,
        country_state,
        address,
        phones,
        hosting,
        ports
      ),
      row.names = c(
        'company_url',
        'ip',
        'netblock',
        'country',
        'country_state',
        'address',
        'phones',
        'hosting',
        'ports'
      )
    )
  company_sum
 
}
 
urls <- c("Vmware.com", "Parallels.com", "Virtualbox.org", "Dosbox.com", "Zophar.net", "Emaculation.com", "Virtual-strategy.com", "Retrogames.com", "Emulationrealm.net", "Bochs.sourceforge.net", "Yellow-bricks.com", "Bbcmicro.co.uk", "Emulators.com", "Xl-project.com", "Ccs64.com" )
 
dfs <- lapply(urls, get_sum_df) 
result <- bind_cols(dfs) 
 
row.names(result) <- c('company_url',
        'ip',
        'netblock',
        'country',
        'country_state',
        'address',
        'phones',
        'hosting',
        'ports'
      )
colnames(result) <- map(result[1,],as.character) %>% unlist()
result <- result[-1,]
knitr::kable(result)
```

Отдельно соберем информацию о веб-технологиях, так как rappalyzer использует непосредственно формат rappalyzer (до этого использовали DataFrame) и самостоятельно строит таблицы.

```{r cache=TRUE}
library(rappalyzer)
rappalyze("Vmware.com")
rappalyze("Parallels.com")
rappalyze("Virtualbox.org")
rappalyze("Dosbox.com")
rappalyze("Zophar.net")
rappalyze("Emaculation.com")
rappalyze("Virtual-strategy.com")
rappalyze("Retrogames.com")
rappalyze("Emulationrealm.net")
rappalyze("Bochs.sourceforge.net")
rappalyze("Yellow-bricks.com")
rappalyze("Bbcmicro.co.uk")
rappalyze("Emulators.com")
rappalyze("Xl-project.com")
rappalyze("Ccs64.com")
```

### Оценка результата.
В результате выполнения данной лабораторной работы я собрал данные о топ 15 сайтах в категории Emulators с помощью автоматизированного решения.    

### Вывод.    
В ходе лабораторной работы я научился автоматизировать сбор данных о доменах.